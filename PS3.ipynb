{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the following binary model\n",
    "$P(Y_{i} = y_{i}) = \\pi_{i}^{y_{i}}*(1-\\pi_{i})^{1-y_{i}}, y_{i} \\in \\{0, 1\\}$   \n",
    "where  \n",
    "$\\pi_{i}(x_{i}, \\beta) = \\frac{e^{x'_{i}\\beta}}{(1 + e^{x'_{i}\\beta})} = \\frac{1}{1+e^{-x'_{i}\\beta}}$\n",
    "\n",
    "1. Simulate this model with the probabilities as described above with the following values:\n",
    "\n",
    "- $n =1000$\n",
    "- $\\beta_0=-2, \\, \\beta_1=0.1,\\, \\, \\, \\beta_2=1$.\n",
    "- $x_{0i}=1 \\, \\forall \\,i$ , $x_{1i}\\sim \\mathcal{U}(18,60), \\, x_{2i}\\sim \\mathcal{B}(0.5)$. \n",
    "\n",
    "2. Estimate $\\beta_0, \\, \\beta_1,\\, \\beta_2$ via maximum likelihood and calculate the standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y_{i} = 1) = \\pi_{i}^{1}*(1-\\pi_{i})^{1-1} = \\pi_{i}$  \n",
    "$\\log(\\frac{\\pi_{i}}{1 - \\pi_{i}}) = x'_{i}\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "pacman::p_load(ggplot2, tidyverse, MASS, caret, ggpubr, maxLik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set.seed()\n",
    "seed <- 40\n",
    "set.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n <- 1000\n",
    "beta <- c(-2, 0.1, 1)\n",
    "X1.min <- 18\n",
    "X1.max <- 60\n",
    "X2.P1 <- 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Data Gnerating function\n",
    "data.gen <- function(n, beta, X1.min, X1.max, X2.P1) {\n",
    "  X0 <- rep(1, n)\n",
    "  X1 <- sort(runif(n, X1.min, X1.max))\n",
    "  X2 <- rbinom(n, 1, X2.P1)\n",
    "  X <- cbind(X0, X1, X2)\n",
    "  logodds <- X %*% beta\n",
    "  pi_x <- 1 / (1 + exp(-logodds))\n",
    "  y <- rbinom(n, 1, prob = pi_x)\n",
    "  data <- cbind.data.frame(X, logodds, pi_x, y)\n",
    "  return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a)\n",
    "# Generate the training data\n",
    "data.train <- data.gen(n, beta, X1.min, X1.max, X2.P1)\n",
    "head(data.train)\n",
    "\n",
    "X.train <- cbind(data.train$X0, data.train$X1, data.train$X2)\n",
    "head(X.train)\n",
    "\n",
    "# Generate the test data\n",
    "data.test <- data.gen(n, beta, X1.min, X1.max, X2.P1) \n",
    "head(data.test)\n",
    "\n",
    "X.test <- cbind(data.test$X0, data.test$X1, data.test$X2)\n",
    "head(X.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table(data.train$y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "# Scatterplot of ys on X1\n",
    "ggplot(data.train, aes(x = X1, y = y, colour = factor(y))) + \n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... ys on X2\n",
    "ggplot(data.train, aes(x = X2, y = y, colour = factor(y))) + \n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of X1\n",
    "ggplot(data.train, aes(X1)) + \n",
    "  geom_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of X2\n",
    "ggplot(data.train, aes(X2)) + \n",
    "  geom_histogram() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Obtain the estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# b)\n",
    "# Estimate betas via ML (Logistic Regression)\n",
    "Logit <- glm(y ~ X1 + X2, data = data.train, family = binomial)\n",
    "summary(Logit)\n",
    "(beta.logit <- Logit$coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared with estimates obtained from manually doing ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y_{i} = y_{i}) = \\pi_{i}^{y_{i}}*(1-\\pi_{i})^{1-y_{i}}$  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L &= P(Y_{1} = y_{1},...Y_{n} = y_{n}) = \\prod_{i = 1}^{n} \\pi_{i}^{y_{i}}*(1-\\pi_{i})^{1-y_{i}} \\\\\n",
    "logL &= \\sum_{i = 1}^{n} y_{i} * \\log(\\pi_{i}) + (1 - y_{i}) * \\log(1 - \\pi_{i})\\\\\n",
    "\\pi_{i} &= \\frac{1}{1 + e^{-X\\beta}}\\\\\n",
    "1 - \\pi_{i} &= \\frac{1}{1 + e^{X\\beta}}\\\\\n",
    "logL &= \\sum_{i = 1}^{n} y_{i}(-log(1 + e^{-X\\beta}) + (1 - y_{i})(-log(1 + e ^ {X\\beta}))\\\\\n",
    "&= \\sum_{i = 1}^{n} -y_{i}(log(1 + e^{-X\\beta}) - (1 - y_{i})(log(1 + e ^ {X\\beta}))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compared with manually doing ML\n",
    "\n",
    "loglike<-function(b) # the likelihood function for the logit model\n",
    "{\n",
    "  ll <- sum(-data.train$y * log(1 + exp(-(X.train %*% b))) - \n",
    "              (1 - data.train$y) * log(1 + exp(X.train %*% b)))\n",
    "  return(ll)\n",
    "}\n",
    "\n",
    "# Initialize estimation procedure\n",
    "estim <- maxBFGS(loglike, finalHessian = TRUE, start = c(0, 0, 1)) \n",
    "beta.ML <- estim$estimate # give out parameter estimates\n",
    "beta.ML\n",
    "(beta.logit <- Logit$coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard Error of the Coefficients\n",
    "estim.hess <- estim$hessian \n",
    "# the optimization routine returns the hessian matrix at the last iteration.\n",
    "Cov <- -(solve(estim.hess))\n",
    "# the covariance matrix is the (negative) inverse of the hessian matrix.\n",
    "sde.ML <- sqrt(diag(Cov))#the standard errors are the square root of the diagonal of the inverse Hessian. \n",
    "sde.ML\n",
    "stdEr(Logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare some data for plotting \n",
    "#### True probabilities and logodds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y_{i} = 1) = \\pi_{i}^{1}*(1-\\pi_{i})^{1-1} = \\pi_{i} = \\frac{1}{1+e^{-x'_{i}\\beta}}$  \n",
    "$\\log(\\frac{\\pi_{i}}{1 - \\pi_{i}}) = x'_{i}\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare some Data for plotting later\n",
    "# \"True\" log odds and probs (never observe in real life)\n",
    "head(data.train)\n",
    "logodds.true.train <- data.train$logodds\n",
    "probs.true.train <- data.train$pi_x\n",
    "\n",
    "# \"True\" log odds and probs for test data\n",
    "head(data.test)\n",
    "logodds.true.test <- data.test$logodds \n",
    "probs.true.test <- data.test$pi_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted Log odds and Probailities, and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{P}(Y_{i} = 1) = \\hat{\\pi_{i}} = \\frac{1}{1+e^{-x'_{i}\\hat{\\beta}}}$  \n",
    "${\\log}(\\frac{\\hat{\\pi_{i}}}{1 - \\hat{\\pi_{i}}}) = x'_{i}\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 95% CI for the estimated probs\n",
    "# \"Manual\" way\n",
    "\n",
    "beta.ML.lower <- beta.ML - 1.96 * sde.ML\n",
    "beta.ML.upper <- beta.ML + 1.96 * sde.ML\n",
    "\n",
    "logodds.ML.train <- X.train %*% beta.ML\n",
    "logodds.ML.train.lower <- X.train %*% beta.ML.lower\n",
    "logodds.ML.train.upper <- X.train %*% beta.ML.upper\n",
    "\n",
    "probs.ML.train <- 1 / (1 + exp(-logodds.ML.train))\n",
    "probs.ML.train.lower <- 1 / (1 + exp(-logodds.ML.train.lower))\n",
    "probs.ML.train.upper <- 1 / (1 + exp(-logodds.ML.train.upper))\n",
    "\n",
    "logodds.ML.test <- X.test %*% beta.ML\n",
    "logodds.ML.test.lower <- X.test %*% beta.ML.lower\n",
    "logodds.ML.test.upper <- X.test %*% beta.ML.upper\n",
    "\n",
    "probs.ML.test <- 1 / (1 + exp(-logodds.ML.test))\n",
    "probs.ML.test.lower <- 1 / (1 + exp(-logodds.ML.test.lower))\n",
    "probs.ML.test.upper <- 1 / (1 + exp(-logodds.ML.test.upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct 95% CI for the estimated probs: \"let-the-machine-do-it\" way\n",
    "# Fit the model again to the data.train\n",
    "logit.train <- predict(Logit, data.train, se = T)\n",
    "\n",
    "logodds.fit.train <- logit.train$fit\n",
    "probs.fit.train <- 1 / (1 + exp(-logodds.fit.train))\n",
    "\n",
    "logodds.lower.train <- logodds.fit.train - 1.96 * logit.train$se.fit # lower bound\n",
    "logodds.upper.train <- logodds.fit.train + 1.96 * logit.train$se.fit # upper bound\n",
    "lower.train <- 1 / (1 + exp(-logodds.lower.train)) \n",
    "upper.train <- 1 / (1 + exp(-logodds.upper.train))\n",
    "\n",
    "# Marginal Effect of Pr(y = 1|X) = p(X) on X1\n",
    "dx1.train <- beta.logit[2] * probs.fit.train * (1 - probs.fit.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the estimated betas to fit the test data\n",
    "logit.test <- predict(Logit, newdata = data.test, se = T)\n",
    "logodds.fit.test <- logit.test$fit\n",
    "\n",
    "# Check if it's correct\n",
    "head(logodds.fit.test)\n",
    "head(X.test %*% beta.logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct 95% CI for the (test) estimated probs \n",
    "lower.test <- logodds.fit.test - 1.96 * logit.test$se.fit # lower bound of log odds\n",
    "upper.test <- logodds.fit.test + 1.96 * logit.test$se.fit # upper bound...\n",
    "probs.fit.test <- 1 / (1 + exp(-logodds.fit.test))\n",
    "lower.test <- 1 / (1 + exp(-lower.test)) \n",
    "upper.test <- 1 / (1 + exp(-upper.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE, AVE and the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MSE and AVE\n",
    "y.pred.train <-  c()\n",
    "y.pred.test <- c()\n",
    "threshold <- 0.65\n",
    "for (i in 1:n) {\n",
    "  if (probs.fit.train[i] >= threshold) {\n",
    "    y.pred.train[i] <- 1\n",
    "  }\n",
    "  else {\n",
    "    y.pred.train[i] <- 0\n",
    "  }\n",
    "  if (probs.fit.test[i] >= threshold) {\n",
    "    y.pred.test[i] <- 1\n",
    "  }\n",
    "  else {\n",
    "    y.pred.test[i] <- 0\n",
    "  }\n",
    "}\n",
    "# Training error\n",
    "(MSE <- sum(y.pred.train != data.train$y) / length(data.train$y))\n",
    "\n",
    "# Testing error\n",
    "(AVE <- sum(y.pred.test != data.test$y) / length(data.test$y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For confusion matrices we factor() the ys for better interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Confusion Matrices we factor the ys\n",
    "y.train.cfm <- factor(data.train$y, levels = c(0, 1), labels = c(\"Pos\", \"Neg\"))\n",
    "y.test.cfm <- factor(data.test$y, levels = c(0, 1), labels = c(\"Pos\", \"Neg\"))\n",
    "y.pred.train.cfm <- factor(y.pred.train, levels = c(0, 1), labels = c(\"Pos\", \"Neg\"))\n",
    "y.pred.test.cfm <- factor(y.pred.test, levels = c(0, 1), labels = c(\"Pos\", \"Neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cfm.train <- table(y.pred.train.cfm, y.train.cfm, dnn = c(\"Predicted\", \"True\")))\n",
    "(addmargins(cfm.train))\n",
    "addmargins(prop.table(cfm.train))\n",
    "addmar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(mosaic.train <- (mosaicplot(table(y.train.cfm, y.pred.train.cfm, dnn = c(\"True\", \"Predicted\")))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(cfm.test <- table(y.pred.test.cfm, y.test.cfm, dnn = c(\"Predicted\", \"True\")))\n",
    "(addmargins(cfm.test))\n",
    "addmargins(prop.table(cfm.test))\n",
    "addmargins(prop.table(cfm.test, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mosaic.test <- (mosaicplot(table(y.test.cfm, y.pred.test, dnn = c(\"True\", \"Predicted\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MSE\n",
    "0.102 + 0.057\n",
    "AVE\n",
    "0.077 + 0.062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the first observation. We'll calculate its log odds, odds and $P(Y_{1} = 1) = \\pi_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{log.odds_{1}} &= x'_{1}\\hat{\\beta} \\\\\n",
    "\\hat{odds_{1}} &= e^{\\hat{log.odds_{1}}} \\\\\n",
    "\\hat{P}(Y_{1} = 1) &= \\frac{1}{1 + e ^ {-\\hat{log.odds_{1}}}} = \\hat{\\pi_{1}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c) Interpretation\n",
    "\n",
    "# Logodss of the first observation and P(Y_1 = 1)\n",
    "logit.train$fit[1] # \"automatic\"\n",
    "X.train[1, ] %*% beta.logit # \"manually\"\n",
    "# Odss of the first obs\n",
    "(odds_1 <- exp(logit.train$fit[1]))\n",
    "# With an odds of 1.76:1 the first observation is in group 1\n",
    "# Or in other words, the probability that Y_1 = 1 is:\n",
    "(pi_x_1 <- 1 / (1 + exp(-logit.train$fit[1]))) # P(Y_1 = 1) is around 64% \n",
    "                                              # gives an odds of ~ 64/36 ~ 1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Effects\n",
    "$\\hat{log.odds_{1}} = x'_{1}\\hat{\\beta} = \\hat{\\beta_{0}} * x_{10} + \\hat{\\beta_{1}} * x_{11} + \\hat{\\beta_{2}} * x_{12}$  \n",
    "One unit increase in $x_{11}$ increases $\\hat{odds_{1}}$ by $e^{\\hat{\\beta_{1}}} - 1 \\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(exp(beta.logit[2]) - 1) * 100\n",
    "logodds_1_new <- beta.logit[1] * X.train[1, 1] +\n",
    "                    beta.logit[2] * (X.train[1, 2] + 1) +\n",
    "                    beta.logit[3] * X.train[1, 3]\n",
    "odds_1_new <- exp(logodds_1_new)\n",
    "(odds_1_new - odds_1) / odds_1 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about $\\frac{\\partial \\pi}{\\partial X_1}?$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_i&= \\frac{1}{1 + e^{-(\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})}} \\\\\n",
    "\\frac{\\partial \\pi_{i}}{\\partial x_{i1}} &= \\frac{-1}{(1 + e^{-(\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})})^2}(-\\beta_1)e^{\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})}\\\\\n",
    "&= \\beta_1\\frac{1}{1 + e^{-(\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})}}\\frac{e^{-(\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})}}{1 + e^{-(\\beta_{0}x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2})}} \\\\\n",
    "&= \\beta_1\\pi_i(1 - \\pi_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Consider again the first observation:  \n",
    "$\\frac{\\partial \\pi_{i}}{\\partial x_{i1}}|x_{11} = \\beta_1\\pi_1(1 - \\pi_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(dx1_1 <- beta.logit[2] * pi_x_1 * (1 - pi_x_1)) * 100 # dx1 of the 1st. obs: from math\n",
    "pi_x_1_new <- 1 / (1 + exp(-logodds_1_new))\n",
    "(pi_x_1_new - pi_x_1) * 100 # dx1 of obs 1: computational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One unit increase in x1 increases the predicted probability by 2.4 percentage points (from math) or 2.37 percentage points practically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) VIsualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data frame for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.data <- data.frame(X1.train = data.train$X1,\n",
    "                       X1.test = data.test$X1,\n",
    "                       X2.train = factor(data.train$X2),\n",
    "                       X2.test = factor(data.test$X2),\n",
    "                       y.train = data.train$y,\n",
    "                       y.test = data.test$y,\n",
    "                       probs.train = probs.fit.train,\n",
    "                       probs.ML.train,\n",
    "                       probs.test = probs.fit.test,\n",
    "                       probs.ML.test,\n",
    "                       probs.true.train,\n",
    "                       probs.true.test,\n",
    "                       upper.train,\n",
    "                       probs.ML.train.upper,\n",
    "                       lower.train,\n",
    "                       probs.ML.train.lower,\n",
    "                       upper.test,\n",
    "                       probs.ML.test.upper,\n",
    "                       lower.test,\n",
    "                       probs.ML.test.lower,\n",
    "                       dx1.train)\n",
    "\n",
    "head(plt.data)  \n",
    "str(plt.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.fit.train, colour = X2.train), cex = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginal effect of X1\n",
    "ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = dx1.train, colour = X2.train), cex = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.fit.train, colour = X2.train), cex = 1.5) +\n",
    "  geom_ribbon(aes(ymin = lower.train,\n",
    "                  ymax = upper.train,\n",
    "                  colour = X2.train),\n",
    "                  alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.fit.train, colour = X2.train), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = lower.train,\n",
    "                  ymax = upper.train,\n",
    "                  colour = X2.train),\n",
    "                  alpha = 0.1) +\n",
    "  geom_point(aes(y = y.train), size = 0.3) +\n",
    "  geom_point(aes(y = probs.true.train), shape = \"x\", cex = 0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.test)) +\n",
    "geom_line(aes(y = probs.fit.test, colour = X2.test), cex = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.test)) +\n",
    "geom_line(aes(y = probs.fit.test, colour = X2.test), cex = 1) +\n",
    "geom_ribbon(aes(ymin = lower.test,\n",
    "                ymax = upper.test,\n",
    "                colour = X2.test),\n",
    "            alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.test)) +\n",
    "geom_line(aes(y = probs.fit.test, colour = X2.test), cex = 1) +\n",
    "geom_ribbon(aes(ymin = lower.test,\n",
    "                ymax = upper.test,\n",
    "                colour = X2.test),\n",
    "            alpha = 0.1) +\n",
    "geom_point(aes(y = y.test), size = 0.3) +\n",
    "geom_point(aes(y = probs.true.test), shape = \"x\", cex = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.ML.train, colour = X2.train), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = probs.ML.train.lower,\n",
    "                  ymax = probs.ML.train.upper,\n",
    "                  colour = X2.train),\n",
    "              alpha = 0.1) +\n",
    "  geom_point(aes(y = y.train), size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.train.plt <- ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.fit.train, colour = X2.train), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = lower.train,\n",
    "                  ymax = upper.train,\n",
    "                  colour = X2.train),\n",
    "              alpha = 0.1) +\n",
    "  geom_point(aes(y = y.train), size = 0.3) \n",
    "\n",
    "ML.train.plt <- ggplot(plt.data, aes(X1.train)) +\n",
    "  geom_line(aes(y = probs.ML.train, colour = X2.train), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = probs.ML.train.lower,\n",
    "                  ymax = probs.ML.train.upper,\n",
    "                  colour = X2.train),\n",
    "              alpha = 0.1) +\n",
    "  geom_point(aes(y = y.train), size = 0.3)\n",
    "\n",
    "logit.test.plt <- ggplot(plt.data, aes(X1.test)) +\n",
    "  geom_line(aes(y = probs.fit.test, colour = X2.test), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = lower.test,\n",
    "                  ymax = upper.test,\n",
    "                  colour = X2.test),\n",
    "              alpha = 0.1) +\n",
    "  geom_point(aes(y = y.test), size = 0.3)\n",
    "\n",
    "ML.test.plt <- ggplot(plt.data, aes(X1.test)) +\n",
    "  \n",
    "  geom_line(aes(y = probs.ML.test, colour = X2.test), cex = 1) +\n",
    "  geom_ribbon(aes(ymin = probs.ML.test.lower,\n",
    "                  ymax = probs.ML.test.upper,\n",
    "                  colour = X2.test),\n",
    "              alpha = 0.1) +\n",
    "  geom_point(aes(y = y.test), size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggarrange(logit.train.plt, ML.train.plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggarrange(logit.test.plt, ML.test.plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "set.seed(seed)\n",
    "nsim <- 100 # number of simulations\n",
    "n <- 1000\n",
    "beta <- beta\n",
    "X1.min <- 18\n",
    "X1.max <- 60\n",
    "X2.P1 <- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data generating function\n",
    "data.gen <- function(n, beta, X1.min, X1.max, X2.P1) {\n",
    "  X0 <- rep(1, n)\n",
    "  X1 <- sort(runif(n, X1.min, X1.max))\n",
    "  X2 <- rbinom(n, 1, X2.P1)\n",
    "  X <- cbind(X0, X1, X2)\n",
    "  logodds <- X %*% beta\n",
    "  pi_x <- 1 / (1 + exp(-logodds))\n",
    "  y <- rbinom(n, 1, prob = pi_x)\n",
    "  data <- cbind.data.frame(X, logodds, pi_x, y)\n",
    "  return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get MSE and AVE from a model\n",
    "geterr <- function(threshold) {\n",
    "  # Generate data\n",
    "  data.train <- data.gen(n, beta, X1.min, X1.max, X2.P1)\n",
    "  data.test <- data.gen(n, beta, X1.min, X1.max, X2.P1) \n",
    "  X.train <- cbind(data.train$X0, data.train$X1, data.train$X2)\n",
    "  X.test <- cbind(data.test$X0, data.test$X1, data.test$X2)\n",
    "  \n",
    "  # Get logistic estimates and fitted values\n",
    "  Logit <- glm(y ~ X1 + X2, data = data.train, family = binomial)\n",
    "  logit.train <- predict(Logit, data.train, se = T)\n",
    "  logodds.fit.train <- logit.train$fit\n",
    "  probs.fit.train <- 1 / (1 + exp(-logodds.fit.train))\n",
    "  \n",
    "  logit.test <- predict(Logit, newdata = data.test, se = T)\n",
    "  logodds.fit.test <- logit.test$fit\n",
    "  probs.fit.test <- 1 / (1 + exp(-logodds.fit.test))\n",
    "  \n",
    "  # MSE and AVE\n",
    "  y.pred.train <-  c()\n",
    "  y.pred.test <- c()\n",
    "  \n",
    "  for (i in 1:n) {\n",
    "    if (probs.fit.train[i] >= threshold) {\n",
    "      y.pred.train[i] <- 1\n",
    "    }\n",
    "    else {\n",
    "      y.pred.train[i] <- 0\n",
    "    }\n",
    "    if (probs.fit.test[i] >= threshold) {\n",
    "      y.pred.test[i] <- 1\n",
    "    }\n",
    "    else {\n",
    "      y.pred.test[i] <- 0\n",
    "    }\n",
    "  }\n",
    "  # Training error\n",
    "  MSE <- sum(y.pred.train != data.train$y) / length(data.train$y)\n",
    "  \n",
    "  # Testing error\n",
    "  AVE <- sum(y.pred.test != data.test$y) / length(data.test$y)\n",
    "  errs <- cbind.data.frame(MSE, AVE)\n",
    "  \n",
    "  return(errs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time for simulations\n",
    "# Simulate and store the interested values\n",
    "errlist <- replicate(nsim, geterr(min(0.7)), simplify = F)\n",
    "MSEs <- c()\n",
    "AVEs <- c()\n",
    "for (i in 1:nsim) {\n",
    "  MSEs[i] <- errlist[[i]]$MSE\n",
    "  AVEs[i] <- errlist[[i]]$AVE\n",
    "}\n",
    "MSEs\n",
    "AVEs\n",
    "plt.data.2 <- cbind.data.frame(sim = 1:nsim, err = MSEs, type = \"MSEs\") %>%\n",
    "  rbind(data.frame(sim = 1:nsim, err = AVEs, type = \"AVEs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(sim, err, colour = type)) + \n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(y = err, colour = type)) + \n",
    "  geom_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(x = err, colour = type)) + \n",
    "  geom_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n <- 1000\n",
    "beta <- beta\n",
    "X1.min <- 18\n",
    "X1.max <- 60\n",
    "X2.P1 <- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data.gen function here for modifications\n",
    "data.gen <- function(n, beta, X1.min, X1.max, X2.P1) {\n",
    "  X0 <- rep(1, n)\n",
    "  X1 <- sort(runif(n, X1.min, X1.max))\n",
    "  X2 <- rbinom(n, 1, X2.P1)\n",
    "  X <- cbind(X0, X1, X2)\n",
    "  logodds <- X %*% beta\n",
    "  pi_x <- 1 / (1 + exp(-logodds))\n",
    "  y <- rep(0, n)\n",
    "  for (i in 1:n) {\n",
    "    if (pi_x[i] >= min(0.5)) {\n",
    "      y[i] <- 1\n",
    "    }\n",
    "    else {\n",
    "      y[i] <- 0\n",
    "    }\n",
    "  }\n",
    "  data <- cbind.data.frame(X, logodds, pi_x, y)\n",
    "  return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for simulations\n",
    "# Simulate and store the interested values\n",
    "errlist <- replicate(nsim, geterr(min(0.5)), simplify = F)\n",
    "MSEs <- c()\n",
    "AVEs <- c()\n",
    "for (i in 1:nsim) {\n",
    "  MSEs[i] <- errlist[[i]]$MSE\n",
    "  AVEs[i] <- errlist[[i]]$AVE\n",
    "}\n",
    "MSEs\n",
    "AVEs\n",
    "plt.data.2 <- cbind.data.frame(sim = 1:nsim, err = MSEs, type = \"MSEs\") %>%\n",
    "  rbind(data.frame(sim = 1:nsim, err = AVEs, type = \"AVEs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(sim, err, colour = type)) + \n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(y = err, colour = type)) + \n",
    "  geom_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(plt.data.2, aes(x = err, colour = type)) + \n",
    "  geom_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
